{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MASTER Classification Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Ednalyn C. De Dios & Michael P. Moran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO List\n",
    "- [ ] minmaxscale tenure_years?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals\n",
    "1. Explain what is driving customers to churn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deliverables\n",
    "1. [ ] A report (jupyter notebook) containing analysis of what is driving customer churn\n",
    "1. [ ] A csv with the customer_id, probability of churn, and the prediction of churn (1=churn, 0=not_churn)\n",
    "1. [ ] A single google slide that illustrates how your model works, including the features being used\n",
    "    - Audience: senior leadership team\n",
    "    - How were the values derived?\n",
    "    - How likely is the model\n",
    "        - to give a high probability of churn when churn doesn't occur,\n",
    "        - to give a low probability of churn when churn occurs, and\n",
    "        - to accurately predict churn.\n",
    "1. [ ] A python script that prepares data such that it can be fed into your model\n",
    "1. [ ] A README.md file that contains a link to your google slides presentation, and instructions for how to use your python script(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why are our customers churning?**\n",
    "\n",
    "- Could the month in which they signed up influence churn? i.e. if a cohort is identified by tenure, is there a cohort or cohorts who have a higher rate of churn than other cohorts? (Plot the rate of churn on a line chart where x is the tenure and y is the rate of churn (customers churned/total customers))\n",
    "- Are there features that indicate a higher propensity to churn? like type of internet service, type of phone service, online security and backup, senior citizens, paying more than x% of customers with the same services, etc.?\n",
    "- Is there a price threshold for specific services where the likelihood of churn increases once price for those services goes past that point? If so, what is that point for what service(s)?\n",
    "- If we looked at churn rate for month-to-month customers after the 12th month and that of 1-year contract customers after the 12th month, are those rates comparable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Deliverables***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. I will also need a report (ipynb) answering the question, \"Why are our customers churning?\" I want to see the analysis you did to answer my questions and lead to your findings. Please clearly call out the questions and answers you are analyzing. E.g. If you find that month-to-month customers churn more, I won't be surprised, but I am not getting rid of that plan. The fact that they churn is not because they can, it's because they can and they are motivated to do so. I want some insight into why they are motivated to do so. I realize you will not be able to do a full causal experiment, but I hope to see some solid evidence of your conclusions.\n",
    "1. I will need you to deliver to me a csv with the customer_id, probability of churn, and the prediction of churn (1=churn, 0=not_churn). I would also like a single goolgle slide that illustrates how your model works, including the features being used, so that I can deliver this to the SLT when they come with questions about how these values were derived. Please make sure you include how likely your model is to give a high probability of churn when churn doesn't occur, to give a low probability of churn when churn occurs, and to accurately predict churn.\n",
    "1. Finally, our development team will need a .py file that will take in a new dataset, (in the exact same form of the one you acquired from telco_churn.customers) and perform all the transformations necessary to run the model you have developed on this new dataset to provide probabilities and predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Dictionary & Domain Knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender\n",
    "\n",
    "| Description | gender\n",
    "| ------------- |:-------------:\n",
    "| Female      | 0\n",
    "| Male      | 1\n",
    "\n",
    "### Phone Service\n",
    "\n",
    "| Phone Service | phone_id\n",
    "| ------------- |:-------------:\n",
    "| No phone      | 0\n",
    "| One Line      | 1\n",
    "| Two+ lines    | 2\n",
    "\n",
    "### Household Type\n",
    "\n",
    "| Partner & Dependents      | household_type_id\n",
    "| -------------             |:-------------:\n",
    "| No partner, no dependents      | 0\n",
    "| Partner, no dependents      | 1\n",
    "| No partner, dependents    | 2\n",
    "| Partner, dependents | 3\n",
    "\n",
    "### Streaming Services\n",
    "\n",
    "| Streaming TV & Streaming Movies      | streaming_services\n",
    "| -------------             |:-------------:\n",
    "| No internet service      | 0\n",
    "| No streaming tv, no streaming movies      | 1\n",
    "| Has streaming tv, no streaming movies    | 2\n",
    "| No streaming tv, has streaming movies | 3\n",
    "| Has streaming tv, has streaming movies | 4\n",
    "\n",
    "### Online Security & Backup\n",
    "\n",
    "| Online Security & Online Backup      | online_security_backup\n",
    "| -------------             |:-------------:\n",
    "| No internet service      | 0\n",
    "| No security, no backup | 1\n",
    "| Has security, no backup | 2\n",
    "| No security, has backup | 3\n",
    "| Has security, has backup | 4\n",
    "\n",
    "\n",
    "### gender\n",
    "| Gender | Value\n",
    "| ------ |:-----:\n",
    "| Female | 0\n",
    "| Male | 1\n",
    "\n",
    "### device_protection\n",
    "| Device Protection | Value\n",
    "| ------ |:-----:\n",
    "| No internet service | 1\n",
    "| No | 0\n",
    "| Yes | 2\n",
    "\n",
    "### tech_support\n",
    "| Tech Support | Value\n",
    "| ------ |:-----:\n",
    "| No internet service | 1\n",
    "| No | 0\n",
    "| Yes | 2\n",
    "\n",
    "### paperless_billing\n",
    "| Paperless Billing | Value\n",
    "| ------ |:-----:\n",
    "| No | 0\n",
    "| Yes | 1\n",
    "\n",
    "### internet_service_type\n",
    "| internet_service_type_id | internet_service_type |\n",
    "| ------ |:-----:\n",
    "|                        1 | DSL                   |\n",
    "|                        2 | Fiber optic           |\n",
    "|                        3 | None                  |\n",
    "\n",
    "### payment types\n",
    "| payment_type_id | payment_type              |\n",
    "| ------ |:-----:\n",
    "|               1 | Electronic check          |\n",
    "|               2 | Mailed check              |\n",
    "|               3 | Bank transfer (automatic) |\n",
    "|               4 | Credit card (automatic)   |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypotheses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts & Questions\n",
    "\n",
    "#### Questions\n",
    "1. What is SLT?\n",
    "#### From the boss\n",
    "1. Could the month in which they signed up influence churn? i.e. if a cohort is identified by tenure, is there a cohort or cohorts who have a higher rate of churn than other cohorts? (Plot the rate of churn on a line chart where x is the tenure and y is the rate of churn (customers churned/total customers))\n",
    "1. Are there features that indicate a higher propensity to churn? like type of internet service, type of phone service, online security and backup, senior citizens, paying more than x% of customers with the same services, etc.?\n",
    "1. Is there a price threshold for specific services where the likelihood of churn increases once price for those services goes past that point? If so, what is that point for what service(s)?\n",
    "1. If we looked at churn rate for month-to-month customers after the 12th month and that of 1-year contract customers after the 12th month, are those rates comparable?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from env import host, user, password\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Use the mysql connector to query telco_churn.customers. Assign the output of that query to the dataframe df. You want to include all the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_db_url(\n",
    "    hostname: str, username: str, password: str, db_name: str\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    return url for accessing a mysql database\n",
    "    \"\"\"\n",
    "    return f\"mysql+pymysql://{username}:{password}@{hostname}/{db_name}\"\n",
    "\n",
    "\n",
    "def get_sql_conn(hostname: str, username: str, password: str, db_name: str):\n",
    "    \"\"\"\n",
    "    return a mysql connection object\n",
    "    \"\"\"\n",
    "    return create_engine(get_db_url(host, user, password, db_name))\n",
    "\n",
    "\n",
    "def df_from_sql(query: str, url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    return a Pandas DataFrame resulting from a sql query\n",
    "    \"\"\"\n",
    "    return pd.read_sql(query, url)\n",
    "\n",
    "\n",
    "def get_telco_data() -> pd.DataFrame:\n",
    "    db = \"telco_churn\"\n",
    "    query = (\"SELECT * \"\n",
    "             f\"FROM customers;\")\n",
    "    url = get_db_url(host, user, password, db)\n",
    "    return df_from_sql(query, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_telco_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Write a function, peekatdata(dataframe), that takes a dataframe as input and computes and returns the following:\n",
    "\n",
    "    - creates dataframe object head_df (df of the first 5 rows) and prints contents to screen\n",
    "    - creates dataframe object tail_df (df of the last 5 rows) and prints contents to screen\n",
    "    - creates tuple object shape_tuple (tuple of (nrows, ncols)) and prints tuple to screen\n",
    "    - creates dataframe object describe_df (summary statistics of all numeric variables) and prints contents to screen.\n",
    "    - prints to screen the information about a DataFrame including the index dtype and column dtypes, non-null values and memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def peekatdata(dataframe):\n",
    "    head_df = dataframe.head()\n",
    "    print(f\"HEAD\\n{head_df}\", end=\"\\n\\n\")\n",
    "\n",
    "    tail_df = dataframe.tail()\n",
    "    print(f\"TAIL\\n{tail_df}\", end=\"\\n\\n\")\n",
    "\n",
    "    shape_tuple = dataframe.shape\n",
    "    print(f\"SHAPE: {shape_tuple}\", end=\"\\n\\n\")\n",
    "\n",
    "    describe_df = dataframe.describe()\n",
    "    print(f\"DESCRIPTION\\n{describe_df}\", end=\"\\n\\n\")\n",
    "\n",
    "    print(f\"INFORMATION\")\n",
    "    dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "peekatdata(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO\n",
    "- [ ] multiple_lines has \"No phone service\" for the last row, which is not the same as yes/no that other columns have.\n",
    "- [ ] convert total_charges column to float. it is an object after reading from sql database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Write a function, df_value_counts(dataframe), that takes a dataframe as input and computes and returns the values by frequency for each variable. Use the rule of thumb for your logic on whether or not to use the bins argument. The function will use a for loop and an in statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_value_counts(dataframe):\n",
    "    for col in df.columns:\n",
    "        n = df[col].unique().shape[0]\n",
    "        col_bins = min(n, 10)\n",
    "        print(f\"{col}:\")\n",
    "        if df[col].dtype in ['int64', 'float64'] and n > 10:\n",
    "            print(df[col].value_counts(bins=col_bins, sort=False))\n",
    "        else:\n",
    "            print(df[col].value_counts())\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_value_counts(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- customer_id has no duplicates\n",
    "- gender is about even\n",
    "- customers are mostly not seniors\n",
    "- about equally split along single/partner\n",
    "- most customers do not have dependents\n",
    "- there are many new and many old customers\n",
    "- overwhelming majority have phone service\n",
    "- closely split along multiple_lines\n",
    "- overwhelming majority have internet service\n",
    "    - more have fiber than DSL\n",
    "    - most do not have online_security\n",
    "    - most do not have online_backup\n",
    "    - most do not have device_protection\n",
    "    - most do not have tech_support\n",
    "- billing\n",
    "    - about evenly split along streaming_tv\n",
    "    - about evenly splot along streaming_movies\n",
    "    - most customers are month-to-month\n",
    "    - most customers are paperless billing\n",
    "    - most customers pay by some form of check\n",
    "    - many customers pay less than $30\n",
    "    - most are pay between 45 and 110 per month\n",
    "- most have not churned\n",
    "    - about 1900 have\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Missing Values:\n",
    "\n",
    "    - Write a function, that returns a dataframe of the column name and the number of missing values and the percentage of missing values (missing records/total records) for each of the columns that have > 0 missing values.\n",
    "\n",
    "   - Document your takeaways. For each variable:\n",
    "\n",
    "        - should you remove the observations with a missing value for that variable?\n",
    "        - should you remove the variable altogether?\n",
    "        - is missing equivalent to 0 (or some other constant value) in the specific case of this variable?\n",
    "        - should you replace the missing values with a value it is most likely to represent (e.g. Are the missing values a result of data integrity issues and should be replaced by the most likely value?)\n",
    "        - Handle the missing values in the way you recommended above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_missing_vals(dataframe):\n",
    "    null_count = dataframe.isnull().sum()\n",
    "    null_percentage = (null_count / dataframe.shape[0]) * 100\n",
    "    empty_count = pd.Series(((dataframe == \" \") | (dataframe == \"\")).sum())\n",
    "    return pd.DataFrame({\"nmissing\": null_count, \"percentage\": null_percentage, \"nempty\": empty_count})\n",
    "\n",
    "# test \n",
    "# print(df_missing_vals(pd.DataFrame({\"col1\": [np.nan, 1, \"\", np.nan, np.nan], \"col2\": [2, \"\", 4, np.nan, 4]})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_missing_vals(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Document your takeaways. For each variable:\n",
    "    - Me: No columns have NaNs\n",
    "    - Me: total_charges has 11 observations with a \" \" or \"\" value\n",
    "        - [x] let's replace the 11 with their monthly_charges * tenure\n",
    "        - Dd: the total_charges with \" \" all had tenure of 0 so they were essentially replaced with zeroes\n",
    "    - should you remove the observations with a missing value for that variable?\n",
    "    - should you remove the variable altogether?\n",
    "    - is missing equivalent to 0 (or some other constant value) in the specific case of this variable?\n",
    "    - should you replace the missing values with a value it is most likely to represent (e.g. Are the missing values a result of data integrity issues and should be replaced by the most likely value?)\n",
    "\n",
    "Handle the missing values in the way you recommended above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_charges'] = np.where(df['total_charges'] == ' ', (df.monthly_charges * df.tenure), df['total_charges'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Transform churn such that \"yes\" = 1 and \"no\" = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['churn'] = df['churn'].map( {'No': 0, 'Yes': 1} ).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Compute a new feature, tenure_year, that is a result of translating tenure from months to years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tenure_year'] = df.tenure / 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Figure out a way to capture the information contained in phone_service and multiple_lines into a single variable of dtype int. Write a function that will transform the data and place in a new column phone_id in df_sql. Be sure you have documented your function and logic well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_phone_id(df):\n",
    "    '''\n",
    "    return DataFrame with a new colum phone_id that combines phone_service and multiple_lines as follows:\n",
    "    \n",
    "    # 0 = no phone service\n",
    "    # 1 = One line\n",
    "    # 2 = Two+ lines\n",
    "    \n",
    "    '''\n",
    "    df_temp = df.copy()\n",
    "    df_temp.loc[(df_temp.phone_service == 'No'), 'phone_id'] = 0\n",
    "    df_temp.loc[(df_temp.phone_service == 'Yes') & (df_temp.multiple_lines == 'No'), 'phone_id'] = 1\n",
    "    df_temp.loc[(df_temp.phone_service == 'Yes') & (df_temp.multiple_lines == 'Yes'), 'phone_id'] = 2\n",
    "    df_temp = df_temp.astype({'phone_id': int})\n",
    "    return df_temp\n",
    "\n",
    "df_sql = add_phone_id(df)\n",
    "# df_sql[['phone_service', 'multiple_lines', 'phone_id']].tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Figure out a way to capture the information contained in dependents and partner into a single variable of dtype int. Transform the data and place in a new column household_type_id in df_sql. Be sure you have documented your function and logic well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_household_type_id(df):\n",
    "    '''\n",
    "    return DataFrame with a new colum household_type_id that combines partner and dependents as follows:\n",
    "    \n",
    "    # 0 = no partner, no dependents\n",
    "    # 1 = has partner, no dependents\n",
    "    # 2 = no partner, has dependents\n",
    "    # 3 = has partner, has dependents\n",
    "    \n",
    "    '''\n",
    "    df_temp = df.copy()\n",
    "    df_temp.loc[(df_temp.partner == 'No')& (df_temp.dependents == 'No'), 'household_type_id'] = 0\n",
    "    df_temp.loc[(df_temp.partner == 'Yes') & (df_temp.dependents == 'No'), 'household_type_id'] = 1\n",
    "    df_temp.loc[(df_temp.partner == 'No') & (df_temp.dependents == 'Yes'), 'household_type_id'] = 2\n",
    "    df_temp.loc[(df_temp.partner == 'Yes') & (df_temp.dependents == 'Yes'), 'household_type_id'] = 3\n",
    "    df_temp = df_temp.astype({'household_type_id': int})\n",
    "    return df_temp\n",
    "\n",
    "df_sql = add_household_type_id(df_sql)\n",
    "# df_sql[[\"partner\", \"dependents\", \"household_type_id\"]].tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Figure out a way to capture the information contained in streaming_tv and streaming_movies into a single variable of dtype int. Transform the data and place in a new column streaming_services in df_sql. Be sure you have documented your function and logic well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_streaming_services(df):\n",
    "    \"\"\"\n",
    "    return DataFrame with a new column streaming_services that combines streaming_tv and streaming_movies columns as follows:\n",
    "    # 0 = no internet service\n",
    "    # 1 = no streaming_tv, no streaming_movies\n",
    "    # 2 = has streaming_tv, no streaming_movies\n",
    "    # 3 = no streaming_tv, has streaming_movies\n",
    "    # 4 = has streaming_tv, has streaming_movies\n",
    "    \"\"\"\n",
    "    df_temp = df.copy()\n",
    "    df_temp.loc[(df_temp.streaming_tv == \"No internet service\") & (df_temp.streaming_movies == 'No internet service'), \"streaming_services\"] = int(0)\n",
    "    df_temp.loc[(df_temp.streaming_tv == \"No\") & (df_temp.streaming_movies == 'No'), \"streaming_services\"] = int(1)\n",
    "    df_temp.loc[(df_temp.streaming_tv == \"Yes\") & (df_temp.streaming_movies == 'No'), \"streaming_services\"] = int(2)\n",
    "    df_temp.loc[(df_temp.streaming_tv == \"No\") & (df_temp.streaming_movies == 'Yes'), \"streaming_services\"] = int(3)\n",
    "    df_temp.loc[(df_temp.streaming_tv == \"Yes\") & (df_temp.streaming_movies == 'Yes'), \"streaming_services\"] = int(4)\n",
    "    df_temp = df_temp.astype({\"streaming_services\": int})\n",
    "    \n",
    "    return df_temp\n",
    "\n",
    "df_sql = add_streaming_services(df_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. Figure out a way to capture the information contained in online_security and online_backup into a single variable of dtype int. Transform the data and place in a new column online_security_backup in df_sql. Be sure you have documented your function and logic well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_online_security_backup(df):\n",
    "    \"\"\"\n",
    "    # 0 = no internet service\n",
    "    # 1 = no online_security, no online_backup\n",
    "    # 2 = has online_security, no online_backup\n",
    "    # 3 = no online_security, has online_backup\n",
    "    # 4 = has online_security, has online_backup\n",
    "    \"\"\"\n",
    "    df_temp = df.copy()\n",
    "    df_temp.loc[(df_temp.online_security == \"No internet service\") & (df_temp.online_backup == \"No internet service\"), \"online_security_backup\"] = 0\n",
    "    df_temp.loc[(df_temp.online_security == \"No\") & (df_temp.online_backup == \"No\"), \"online_security_backup\"] = 1\n",
    "    df_temp.loc[(df_temp.online_security == \"Yes\") & (df_temp.online_backup == \"No\"), \"online_security_backup\"] = 2\n",
    "    df_temp.loc[(df_temp.online_security == \"No\") & (df_temp.online_backup == \"Yes\"), \"online_security_backup\"] = 3\n",
    "    df_temp.loc[(df_temp.online_security == \"Yes\") & (df_temp.online_backup == \"Yes\"), \"online_security_backup\"] = 4\n",
    "    df_temp = df_temp.astype({\"online_security_backup\": int})\n",
    "    \n",
    "    return df_temp\n",
    "\n",
    "df_sql = add_online_security_backup(df_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Data Split\n",
    "\n",
    "    - Split data into train (70%) & test (30%) samples. You should end with 2 data frames: train_df and test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcols = ['gender', 'senior_citizen', 'internet_service_type_id', 'device_protection', 'tech_support',\n",
    "         'contract_type_id', 'paperless_billing', 'payment_type_id', 'monthly_charges', 'total_charges',\n",
    "         'tenure_year', 'phone_id', 'household_type_id', 'streaming_services', 'online_security_backup']\n",
    "X = df_sql[xcols]\n",
    "y = df_sql[['churn']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .30, random_state = 123, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Variable Encoding\n",
    "\n",
    "    - Write an encoder (fit and transform on train_df) for each non-numeric variable. Use that encoder object to transform on test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_encode = [\"gender\", \"device_protection\", \"tech_support\", \"paperless_billing\"]\n",
    "encoders = {}\n",
    "for col in to_encode:\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(X_train[col])\n",
    "    X_train[col] = encoder.transform(X_train[col])\n",
    "    X_test[col] = encoder.transform(X_test[col])\n",
    "    encoders[col] = encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. Numeric Scaling\n",
    "\n",
    "    - Fit a min_max_scaler to train_df. Transform monthly_charges and total_charges variables in train_df using the scaler. Then use the scaler object to transform test_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train[[\"monthly_charges\", \"total_charges\"]])\n",
    "X_train[[\"monthly_charges\", \"total_charges\"]] = scaler.transform(X_train[[\"monthly_charges\", \"total_charges\"]])\n",
    "X_test[[\"monthly_charges\", \"total_charges\"]] = scaler.transform(X_test[[\"monthly_charges\", \"total_charges\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "### Deliverable\n",
    "\n",
    "*I will also need a report (ipynb) answering the question, **\"Why are our customers churning?\"** I want to see the analysis you did to answer my questions and lead to your findings. Please clearly call out the questions and answers you are analyzing. E.g. If you find that month-to-month customers churn more, I won't be surprised, but I am not getting rid of that plan. The fact that they churn is not because they can, it's because they can and they are motivated to do so. I want some insight into why they are motivated to do so. I realize you will not be able to do a full causal experiment, but I hope to see some solid evidence of your conclusions.*\n",
    "\n",
    "1. Could the month in which they signed up influence churn? i.e. if a cohort is identified by tenure, is there a cohort or cohorts who have a higher rate of churn than other cohorts? (Plot the rate of churn on a line chart where x is the tenure and y is the rate of churn (customers churned/total customers))\n",
    "\n",
    "2. Are there features that indicate a higher propensity to churn? like type of internet service, type of phone service, online security and backup, senior citizens, paying more than x% of customers with the same services, etc.?\n",
    "\n",
    "3. Is there a price threshold for specific services where the likelihood of churn increases once price for those services goes past that point? If so, what is that point for what service(s)?\n",
    "\n",
    "4. If we looked at churn rate for month-to-month customers after the 12th month and that of 1-year contract customers after the 12th month, are those rates comparable?\n",
    "\n",
    "5. Controlling for services (phone_id, internet_service_type_id, online_security_backup, device_protection, tech_support, and contract_type_id), is the mean monthly_charges of those who have churned significantly different from that of those who have not churned?\n",
    "\n",
    "6. How much of monthly_charges can be explained by internet_service_type? (hint: correlation test). State your hypotheses and your conclusion clearly.\n",
    "\n",
    "7. How much of monthly_charges can be explained by internet_service_type + phone service type (0, 1, or multiple lines). State your hypotheses and your conclusion clearly.\n",
    "\n",
    "8. Create visualizations exploring the interactions of variables (independent with independent and independent with dependent). The goal is to identify features that are related to churn, identify any data integrity issues, understand 'how the data works', e.g. we may find that all who have online services also have device protection. In that case, we don't need both of those. (The visualizations done in your analysis for questions 1-5 count towards the requirements below)\n",
    "\n",
    "    - Each independent variable (except for customer_id) must be visualized in at least two plots, and at least 1 of those compares the independent variable with the dependent variable.\n",
    "\n",
    "    - For each plot where x and y are independent variables, add a third dimension (where possible), of churn represented by color.\n",
    "\n",
    "    - Use subplots when plotting the same type of chart but with different variables.\n",
    "\n",
    "    - Adjust the axes as necessary to extract information from the visualizations (adjusting the x & y limits, setting the scale where needed, etc.)\n",
    "\n",
    "    - Add annotations to at least 5 plots with a key takeaway from that plot.\n",
    "\n",
    "    - Use plots from matplotlib, pandas and seaborn.\n",
    "\n",
    "    - Use each of the following:\n",
    "\n",
    "        - sns.heatmap\n",
    "        - pd.crosstab (with color)\n",
    "        - pd.scatter_matrix\n",
    "        - sns.barplot\n",
    "        - sns.swarmplot\n",
    "        - sns.pairplot\n",
    "        - sns.jointplot\n",
    "        - sns.relplot or plt.scatter\n",
    "        - sns.distplot or plt.hist\n",
    "        - sns.boxplot\n",
    "        - plt.plot\n",
    "        \n",
    "    - Use at least one more type of plot that is not included in the list above.\n",
    "    \n",
    "\n",
    "9. What can you say about each variable's relationship to churn, based on your initial exploration? If there appears to be some sort of interaction or correlation, assume there is no causal relationship and brainstorm (and document) ideas on reasons there could be correlation.\n",
    "\n",
    "    - phone_id\n",
    "    - internet_service_type_id\n",
    "    - online_security_backup\n",
    "    - device_protection\n",
    "    - tech_support\n",
    "    - contract_type_id\n",
    "    - senior_citizen\n",
    "    - tenure\n",
    "    - tenure_year\n",
    "    - monthly_charges\n",
    "    - total_charges\n",
    "    - payment_type_id\n",
    "    - paperless_billing\n",
    "    - contract_type_id\n",
    "    - gender\n",
    "    \n",
    "   \n",
    "10. Summarize your conclusions, provide clear answers to the specific questions, and summarize any takeaways/action plan from the work above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling\n",
    "\n",
    "1. Feature Selection: Are there any variables that seem to provide limited to no additional information? If so, remove those and assign the new limited dataframe to train_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Train (fit, transform, evaluate) a logistic regression model varying your meta-parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compare evaluation metrics across all the models, and select the best performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Test the final model (transform, evaluate) on your out-of-sample data (test_df). Summarize the performance. Interpret your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Delivery\n",
    "\n",
    "1. I will need you to deliver to me a csv with the customer_id, probability of churn, and the prediction of churn (1=churn, 0=not_churn). I would also like a single google slide that illustrates how your model works, including the features being used, so that I can deliver this to the SLT when they come with questions about how these values were derived. Please make sure you include how likely your model is to give a high probability of churn when churn doesn't occur, to give a low probability of churn when churn occurs, and to accurately predict churn.\n",
    "\n",
    "1. Finally, our development team will need a .py file that will take in a new dataset, (in the exact same form of the one you acquired from telco_churn.customers) and perform all the transformations necessary to run the model you have developed on this new dataset to provide probabilities and predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
